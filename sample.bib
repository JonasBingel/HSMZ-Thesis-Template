@book{ertelIntroductionArtificialIntelligence2017,
  title = {Introduction to {{Artificial Intelligence}}},
  author = {Ertel, Wolfgang},
  date = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-58487-4},
  isbn = {978-3-319-58487-4},
  langid = {english},
}

@book{russellArtificialIntelligenceModern2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2021},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition},
  publisher = {{Pearson}},
  location = {{Hoboken}},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  isbn = {978-0-13-461099-3},
  keywords = {Artificial intelligence},
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {Reinforcement learning},
}

@incollection{tesauroTemporalDifferenceLearning1992a,
  title = {Temporal {{Difference Learning}} of {{Backgammon Strategy}}},
  booktitle = {Machine {{Learning Proceedings}} 1992},
  author = {Tesauro, Gerald},
  date = {1992},
  pages = {451--457},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-247-2.50063-2},
  abstract = {This paper presents a case study in which the TD(A) algorithm for training connectionist networks, proposed in (Sutton, 1988), is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex nontrivial task. It is found that, with zero knowledge built in, networks are able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. The hidden units in these network have apparently discovered useful features, a longstanding goal of computer games research. Furthermore, when a set of handcrafted features is added to the input representation, the resulting networks reach a near-expert level of performance, and have achieved good results in tests against worldclass human play.},
  isbn = {978-1-55860-247-2},
  langid = {english},
}

@incollection{szitaReinforcementLearningGames2012,
  title = {Reinforcement {{Learning}} in {{Games}}},
  booktitle = {Reinforcement {{Learning}}},
  author = {Szita, István},
  editor = {Wiering, Marco and van Otterlo, Martijn},
  options = {useprefix=true},
  date = {2012},
  series = {Adaptation, {{Learning}}, and {{Optimization}}},
  volume = {12},
  pages = {539--577},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-27645-3_17},
  abstract = {Reinforcement learning and games have a long and mutually beneficial common history. From one side, games are rich and challenging domains for testing reinforcement learning algorithms. From the other side, in several games the best computer players use reinforcement learning. The chapter begins with a selection of games and notable reinforcement learning implementations. Without any modifications, the basic reinforcement learning algorithms are rarely sufficient for high-level gameplay, so it is essential to discuss the additional ideas, ways of inserting domain knowledge, implementation decisions that are necessary for scaling up. These are reviewed in sufficient detail to understand their potentials and their limitations. The second part of the chapter lists challenges for reinforcement learning in games, together with a review of proposed solution methods. While this listing has a game-centric viewpoint, and some of the items are specific to games (like opponent modelling), a large portion of this overview can provide insight for other kinds of applications, too. In the third part we review how reinforcement learning can be useful in game development and find its way into commercial computer games. Finally, we provide pointers for more in-depth reviews of specific games and solution approaches.},
  isbn = {978-3-642-27645-3},
  langid = {english},
}

@thesis{allisSearchingSolutionsGames1994,
  title = {Searching for solutions in games and artificial intelligence},
  author = {Allis, Louis Victor},
  date = {1994},
  institution = {{Ponsen \& Looijen}},
  location = {{Wageningen}},
  isbn = {9789090074887},
  langid = {Summary in Dutch},
  annotation = {OCLC: 60586781},
}

@article{crowleyFlexibleStrategyUse1993,
  title = {Flexible {{Strategy Use}} in {{Young Children}}'s {{Tic-Tac-Toe}}},
  author = {Crowley, Kevin and Siegler, Robert S.},
  date = {1993-10},
  journaltitle = {Cognitive Science},
  volume = {17},
  number = {4},
  pages = {531--561},
  issn = {03640213},
  doi = {10.1207/s15516709cog1704_3},
  url = {http://doi.wiley.com/10.1207/s15516709cog1704_3},
  urldate = {2022-01-02},
  langid = {english},
}

@unpublished{block-berlitzm.ProInformatikFunktionaleProgrammierung2009,
  type = {Vorlesung},
  title = {ProInformatik - Funktionale Programmierung: Vom Amateur zum Großmeister - von Spielbäumen und anderen Wäldern},
  author = {{Block-Berlitz, M.}},
  year = {2009},
  url = {https://www.inf.fu-berlin.de/lehre/SS09/PI02/docs/MinMax.pdf},
  urldate = {2022-02-22},
  langid = {german},
  venue = {{Freie Universität Berlin SoSe 2009}},
}


@thesis{watkinsLearningDelayedRewards1989,
  title = {Learning {{From Delayed Rewards}}},
  author = {Watkins, Christopher},
  date = {1989-01-01},
  institution = {{King's College London}},
  abstract = {Photocopy. Supplied by British Library. Thesis (Ph. D.)--King's College, Cambridge, 1989.},
}

@online{IrelandComparisonThereFundamental,
  title = {Comparison - {{Is}} There a Fundamental Difference between an Environment Being Stochastic and Being Partially Observable?},
  author = {Ireland, David},
  url = {https://ai.stackexchange.com/a/33889/51242},
  urldate = {2022-01-19},
  date = {2021},
  organization = {{Artificial Intelligence Stack Exchange}},
}

@book{millingtonArtificialIntelligenceGames2009,
  title = {Artificial Intelligence for Games},
  author = {Millington, Ian and Funge, John David},
  date = {2009},
  edition = {2nd ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  location = {{Burlington, MA}},
  isbn = {978-0-12-374731-0},
  pagetotal = {870},
  keywords = {Artificial intelligence,Computer animation,Computer games,Programming},
  annotation = {OCLC: ocn319064669},
}

@article{v.neumannZurTheorieGesellschaftsspiele1928,
  title = {Zur Theorie der Gesellschaftsspiele},
  author = {v. Neumann, J.},
  options = {useprefix=true},
  date = {1928-12},
  journaltitle = {Mathematische Annalen},
  shortjournal = {Math. Ann.},
  volume = {100},
  number = {1},
  pages = {295--320},
  issn = {0025-5831, 1432-1807},
  doi = {10.1007/BF01448847},
  langid = {german}
}
